
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Modelling Random Variables</title>
    
    <link rel="stylesheet" href="_static/default.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '1.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="_static/sidebar.js"></script>

        <script src="http://sagecell.sagemath.org/static/jquery.min.js"></script>
        <script src="http://sagecell.sagemath.org/static/embedded_sagecell.js"></script>

        <script>sagecell.makeSagecell({inputLocation: ".sage"});</script>

        <style type="text/css">
                .sagecell .CodeMirror-scroll {
                        overflow-y: hidden;
                        overflow-x: auto;
                }
                .sagecell .CodeMirror {
                        height: auto;
                }
        </style>

    
    <link rel="top" title="Chaospy Documentation" href="index.html" />
    <link rel="next" title="Polynomial Chaos Expansions" href="._chaospy004.html" />
    <link rel="prev" title="A Glimpse of Chaospy in Action" href="._chaospy002.html" />
 
  
       <style type="text/css">
         div.admonition {
           background-color: whiteSmoke;
           border: 1px solid #bababa;
         }
       </style>
      </head>
    
  <body role="document">
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="._chaospy004.html" title="Polynomial Chaos Expansions"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="._chaospy002.html" title="A Glimpse of Chaospy in Action"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">Chaospy Documentation</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="modelling-random-variables">
<span id="sec-dist"></span><h1>Modelling Random Variables<a class="headerlink" href="#modelling-random-variables" title="Permalink to this headline">¶</a></h1>
<div class="section" id="rosenblatt-transformation">
<span id="sec-rosenblatt"></span><h2>Rosenblatt Transformation<a class="headerlink" href="#rosenblatt-transformation" title="Permalink to this headline">¶</a></h2>
<p>Numerical methods for uncertainty quantification need to generate
pseudo-random realizations</p>
<div class="math">
\[\{\bm Q_k\}_{k\in I_K} \qquad I_K = \{1,\dots,K\},\]</div>
<p>from the density <span class="math">\(p_{\bm Q}\)</span>.  Each <span class="math">\(\bm Q\in \{\bm Q_k\}_{k\in I_K}\)</span>
is multivariate with the number of dimensions <span class="math">\(D&gt;1\)</span>.  Generating
realizations from a given density <span class="math">\(p_{\bm Q}\)</span> is often non-trivial, at
least when <span class="math">\(D\)</span> is large.  A very common assumption made in uncertainty
quantification is that each dimension in <span class="math">\(\bm Q\)</span> consists of
stochastically independent components.  Stochastic independence allows
for a joint sampling scheme to be reduced to a series of univariate
samplings, drastically reducing the complexity of generating a sample
<span class="math">\(\bm Q\)</span>.</p>
<p>Unfortunately, the assumption of independence does not always hold in
practice.  We have examples from many research fields where stochastic
dependence must be assumed, including modelling of climate
<a class="reference internal" href="._chaospy005.html#ref10" id="id1">[Ref10]</a>, iron-ore minerals <a class="reference internal" href="._chaospy005.html#ref11" id="id2">[Ref11]</a>,
finance <a class="reference internal" href="._chaospy005.html#ref12" id="id3">[Ref12]</a>, and ion channel densities in
detailed neuroscience models <a class="reference internal" href="._chaospy005.html#ref13" id="id4">[Ref13]</a>.  There also
exists examples where introducing dependent random variables is
beneficial for the modelling process, even though the original input
was stochastically independent <a class="reference internal" href="._chaospy005.html#ref14" id="id5">[Ref14]</a>.  In
any cases, modelling of stochastically dependent variables are
required to perform uncertainty quantification adequately.  A strong
feature of Chaospy is its support for stochastic dependence.</p>
<p>All random samples are in Chaospy generated using Rosenblatt
transformations <span class="math">\(T_{\bm Q}\)</span> <a class="reference internal" href="._chaospy005.html#ref15" id="id6">[Ref15]</a>.  It allows
for a random variable <span class="math">\(\bm U\)</span>, generated uniformly on a unit hypercube
<span class="math">\([0,1]^D\)</span>, to be transformed into <span class="math">\(\bm Q = T^{-1}_{\bm Q}(\bm U)\)</span>,
which behaves as if it were drawn from the density <span class="math">\(p_{\bm Q}\)</span>.  It is
easy to generate pseudo-random samples from a uniform distribution,
and the Rosenblatt transformation can then be used as a method for
generating samples from arbitrary densities.</p>
<p>The Rosenblatt transformation can be derived as follows.  Consider a
probability decomposition, for example for a bivariate random variable
<span class="math">\(\bm Q=(Q_0, Q_1)\)</span>:</p>
<div class="math" id="equation-eq_bivariate">
<span id="eq-eq-bivariate"></span><span class="eqno">(1)</span>\[         p_{Q_0,Q_1}(q_0,q_1) =
         p_{Q_0}(q_0) p_{Q_1\mid Q_0}(q_1 \mid q_0),\]</div>
<p>were <span class="math">\(p_{Q_0}\)</span> is an marginal density function, and
<span class="math">\(p_{Q_1\mid Q_0}\)</span> is a conditional density.
For the multivariate case, the density decomposition
will have the form</p>
<div class="math" id="equation-eq_decompostion">
<span id="eq-eq-decompostion"></span><span class="eqno">(2)</span>\[         p_{\boldsymbol{Q}}(\bm q) =
         \prod_{d=0}^{D-1} p_{Q^{\prime}_d}(q^{\prime}_d),\]</div>
<p>where</p>
<div class="math" id="equation-eq_notation">
<span id="eq-eq-notation"></span><span class="eqno">(3)</span>\[         Q^\prime_d = Q_d \mid Q_0, \dots, Q_{d-1} \qquad
         q^\prime_d = q_d \mid q_0,\dots,q_{d-1}\]</div>
<p>denotes that <span class="math">\(Q_d\)</span> and <span class="math">\(q_d\)</span> are dependent on all components with
lower indices.
A forward Rosenblatt transformation can then be defined as</p>
<div class="math" id="equation-eq_forward">
<span id="eq-eq-forward"></span><span class="eqno">(4)</span>\[         T_{\bm Q}(\bm q) =
         (F_{Q_0^{\prime}}(q_0^{\prime}),
         \dots,F_{Q_{D-1}^{\prime}}(q_{D-1}^{\prime})),\]</div>
<p>where <span class="math">\(F_{Q_d^{\prime}}\)</span> is the cumulative distribution function:</p>
<div class="math" id="equation-eq_general_cdf">
<span id="eq-eq-general-cdf"></span><span class="eqno">(5)</span>\[         F_{Q_d^{\prime}}(q_d^{\prime}) =
         \int_{-\infty}^{q_d} \!\!
         p_{Q_d^{\prime}}(r\mid q_0,\dots,q_{d-1})
         \ud r.\]</div>
<p>This transformation is bijective, so it is always possible to define
the inverse Rosenblatt transformation <span class="math">\(T_{\bm Q}^{-1}\)</span> in a similar
fashion.</p>
</div>
<div class="section" id="numerical-estimation-of-inverse-rosenblatt-transformations">
<span id="sec-invrosenblatt"></span><h2>Numerical Estimation of Inverse Rosenblatt Transformations<a class="headerlink" href="#numerical-estimation-of-inverse-rosenblatt-transformations" title="Permalink to this headline">¶</a></h2>
<p>To implement the Rosenblatt transformation in practice, we need to
identify the inverse transform <span class="math">\(T_{\bm Q}^{-1}\)</span>.  Unfortunately,
<span class="math">\(T_{\bm Q}\)</span> is often non-linear without a closed-form formula, making
analytical calculations of the transformation&#8217;s inverse difficult.  In
the scenario where we do not have a symbolic representation of the
inverse transformation, a numerical scheme has to be employed.  To the
authors&#8217; knowledge, there are no standards for defining such a
numerical scheme.  The following paragraphs therefore describe our
proposed method for calculating the inverse transformation
numerically.</p>
<p>The problem of calculating the inverse transformation <span class="math">\(T^{-1}_{\bm
Q}\)</span> can, by decomposing the definition of the forward Rosenblatt
transformation in <a href="#equation-eq_forward">(4)</a>, be reformulated as</p>
<div class="math">
\[\begin{split}F_{Q^\prime_d}^{-1}(u\mid q_0,\dots,q_{d-1}) &amp;=
\left\{ r : F_{Q^\prime_d}(r\mid q_0,\dots,q_{d-1})=u \right\}
\qquad d=0,\dots,D-1.\end{split}\]</div>
<p>In other words, the challenge of calculating the inverse
transformation can be reformulated as a series of one dimensional
root-finding problems.  In Chaospy, these roots are found by employing
a Newton-Raphson scheme.  However, to ensure convergence, the scheme
is coupled with a bisection method.  The bisection method is
applicable here since the problem is one-dimensional and the functions
of interest are by definition monotone.  When the Newton-Raphson
method fails to converge at an increment, a bisection step gives the
Newton-Raphson a new start location away from the previous
location. This algorithm ensures fast and reliable convergence towards
the root.</p>
<p>The Newton-Raphson-bisection hybrid method is implemented as follows.
The initial values are the lower and upper bounds <span class="math">\([lo_0, up_0]\)</span>.  If
<span class="math">\(p_{Q^{\prime}_d}\)</span> is unbound, the interval is selected such that it
approximately covers the density.  For example for a standard normal
random variable, which is unbound, the interval <span class="math">\([-7.5,7.5]\)</span> will
approximately cover the whole density with an error about <span class="math">\(10^{-14}\)</span>.
The algorithm starts with a Newton-Raphson increment, using the
initial value <span class="math">\(r_0=(up_0-lo_0) u + lo_0\)</span>:</p>
<div class="math" id="equation-eq_newton">
<span id="eq-eq-newton"></span><span class="eqno">(6)</span>\[         r_{k+1} = r_k - \frac{F_{Q^\prime_d}
         (r_k\mid q_0,\dots,q_{d-1})-u}{
         p_{Q^\prime_d}
         (r_k\mid q_0,\dots,q_{d-1})},\]</div>
<p>where the density <span class="math">\(p_{Q^\prime_d}\)</span> can be approximated using finite
differences.
If the new value does not fall in the interval <span class="math">\([lo_k, up_k]\)</span>, this
proposed value is rejected, and is instead replaced with a
bisection increment:</p>
<div class="math" id="equation-eq_bisection">
<span id="eq-eq-bisection"></span><span class="eqno">(7)</span>\[         r_{k+1} = \frac{up_k+lo_k}{2}.\]</div>
<p>In either case, the bounds are updated according to</p>
<div class="math">
\[\begin{split}(lo_{k+1},up_{k+1}) =
\begin{cases}
    (lo_{k}, r_{k+1})
    F_{Q^\prime_d}(r_{k+1}\mid q_0,\dots,q_{d-1}) &gt; u\end{split}\]</div>
<div class="math" id="equation-eq_bounds">
<span id="eq-eq-bounds"></span><span class="eqno">(8)</span>\[\begin{split}             (r_{k+1}, up_{k})
             F_{Q^\prime_d}(r_{k+1}\mid q_0,\dots,q_{d-1}) &lt; u
         \end{cases}\end{split}\]</div>
<p>The algorithm repeats the steps in <a href="#equation-eq_newton">(6)</a>,
<a href="#equation-eq_bisection">(7)</a> and <a href="#equation-eq_bounds">(8)</a>, until the residual
<span class="math">\(|F_{Q^\prime_d}(r_k\mid q_0,\dots,q_{d-1})-u|\)</span> is sufficiently
small.</p>
<p>The described algorithm overcomes one of the challenges of
implementing Rosenblatt transformations in practice: how to calculate
the inverse transformation.  Another challenge is how to construct a
transformation in the first place.  This is the topic of the next
section.</p>
</div>
<div class="section" id="constructing-distributions">
<span id="sec-variable"></span><h2>Constructing Distributions<a class="headerlink" href="#constructing-distributions" title="Permalink to this headline">¶</a></h2>
<p>The backbone of distributions in Chaospy is the Rosenblatt
transformation <span class="math">\(T_{\bm Q}\)</span>.  The method, as described in the previous
section, assumes that <span class="math">\(p_{\bm Q}\)</span> is known to be able to perform the
transformation and its inverse.  In practice, however, we first need
to construct <span class="math">\(p_{\bm Q}\)</span>, before the transformation can be used.  This
can be a challenging task, but in Chaospy a lot of effort has been put
into constructing novel tools for making the process as flexible and
painless as possible.  In essence, users can create their own custom
multivariate distributions using a new methodology as described next.</p>
<p>Following the definition in <a href="#equation-eq_forward">(4)</a>, each Rosenblatt
transformation consists of a collection of conditional distributions.
We express all conditionality through distribution parameters.  For
example, the location parameter of a normal distribution can be set to
be uniformly distributed, say on <span class="math">\([-1,1]\)</span>.  The following interactive
Python code defines a normal variable with a normally distributed
mean:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">uniform</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="n">lo</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">up</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">normal</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">uniform</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
<p>We now have two stochastic variables, <code class="docutils literal"><span class="pre">uniform</span></code> and <code class="docutils literal"><span class="pre">normal</span></code>,
whose joint bivariate distribution can be constructed
through the <code class="docutils literal"><span class="pre">cp.J</span></code> function:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">joint</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">J</span><span class="p">(</span><span class="n">uniform</span><span class="p">,</span> <span class="n">normal</span><span class="p">)</span>
</pre></div>
</div>
<p>The software will, from this minimal formulation, try to sort out
the dependency ordering and construct the full Rosenblatt
transformation.
The only requirement is that a decomposition as
in <a href="#equation-eq_decompostion">(2)</a> is in fact possible.
The result is a fully functioning forward and inverse Rosenblatt
transformation. The following code evaluates the forward
transformation (the density)
at <span class="math">\((1,0.9)\)</span>, the inverse transformation at <span class="math">\((0.4, 0.6)\)</span>,
and draws a random sample from the joint distribution:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">joint</span><span class="o">.</span><span class="n">fwd</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">])</span>
<span class="go">[ 1.          0.15865525]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">joint</span><span class="o">.</span><span class="n">inv</span><span class="p">([</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">])</span>
<span class="go">[-0.2        -0.17466529]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">joint</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
<span class="go">[-0.05992158 -0.07456064]</span>
</pre></div>
</div>
<p>Distributions in higher dimensions are trivially obtained by including more
arguments to the <code class="docutils literal"><span class="pre">cp.J</span></code> function.</p>
<p>As an alternative to the explicit formulation of dependency through
distribution parameters, it is also possible to construct dependencies
implicitly through arithmetic operators.  For example, it is possible
to recreate the example above using addition of stochastic variables
instead of letting a distribution parameter be stochastic. More
precisely, we have a uniform variable on <span class="math">\([-1,1]\)</span> and a normally
distributed variable with location at <span class="math">\(x=0\)</span>. Adding the uniform
variable to the normal variable creates a new normal variable with
stochastic location:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">uniform</span> <span class="o">=</span> <span class="n">Uniform</span><span class="p">(</span><span class="n">lo</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">up</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">normal0</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">normal</span> <span class="o">=</span> <span class="n">normal0</span> <span class="o">+</span> <span class="n">uniform</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">joint</span> <span class="o">=</span> <span class="n">J</span><span class="p">(</span><span class="n">uniform</span><span class="p">,</span> <span class="n">normal</span><span class="p">)</span>
</pre></div>
</div>
<p>As before, the software automatically sorts the dependency
ordering from the context.
Here, since the uniform variable is present as first argument, the
software recognises the second argument as a normal distribution,
conditioned on the uniform distribution, and not the other way
around.</p>
<p>Another favorable feature in Chaospy is that multiple transformations
can be stacked on top of each other.  For example, consider the
example of a multivariate log-normal random variable <span class="math">\(\bm Q\)</span> with
three dependent components.  (Let us ignore for a moment the fact that
Chaospy already offers such a distribution.) Trying to decompose this
distribution is a very cumbersome task if performed manually.
However, this process can be drastically simplified through variable
transformations, for which Chaospy has strong support.  A log-normal
distribution, for example, can be expressed as</p>
<div class="math">
\[\bm Q = e^{\bm Z L + \bm b},\]</div>
<p>where <span class="math">\(\bm Z\)</span> are standard normal variables, and <span class="math">\(L\)</span> and <span class="math">\(\bm b\)</span>
are predefined matrix and vector, respectively.
To implement this particular transformation, we only
have to write</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">Z</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">J</span><span class="p">(</span><span class="n">cp</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">cp</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">cp</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span> <span class="o">=</span> <span class="n">e</span><span class="o">**</span><span class="p">(</span><span class="n">Z</span><span class="o">*</span><span class="n">L</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
<p>The resulting distribution is fully functional multivariate
log-normal, assuming <span class="math">\(L\)</span> and <span class="math">\(\bm b\)</span> are properly defined.</p>
<p>One obvious prerequisite for using univariate distributions to create
conditionals and multivariate distributions, is the availability of
univariate distributions.  Since the univariate distribution is the
fundamental building block, Chaospy offers a large collection of 64
univariate distributions.  They are all listed in table below.  The
titles &#8216;D&#8217;, &#8216;T&#8217; and &#8216;C&#8217; represents Dakota, Turns and Chaospy
respectively. The elements &#8216;y&#8217; and &#8216;n&#8217; represent the answers &#8216;yes&#8217; and
&#8216;no&#8217; indicating if the distribution is supported or not.  The list
shows that Dakota&#8217;s support is limited to 11 distributions, and Turns
has a collection of 26 distributions.</p>
<table border="1" class="docutils">
<colgroup>
<col width="88%" />
<col width="4%" />
<col width="4%" />
<col width="4%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head"><strong>Distribution</strong></th>
<th class="head">D</th>
<th class="head">T</th>
<th class="head">C</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Alpha</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-odd"><td>Anglit</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-even"><td>Arcsinus</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-odd"><td>Beta</td>
<td>y</td>
<td>y</td>
<td>y</td>
</tr>
<tr class="row-even"><td>Brandford</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-odd"><td>Burr</td>
<td>n</td>
<td>y</td>
<td>y</td>
</tr>
<tr class="row-even"><td>Cauchy</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-odd"><td>Chi</td>
<td>n</td>
<td>y</td>
<td>y</td>
</tr>
<tr class="row-even"><td>Chi-Square</td>
<td>n</td>
<td>y</td>
<td>y</td>
</tr>
<tr class="row-odd"><td>Double Gamma</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-even"><td>Double Weibull</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-odd"><td>Epanechnikov</td>
<td>n</td>
<td>y</td>
<td>y</td>
</tr>
<tr class="row-even"><td>Erlang</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-odd"><td>Exponential</td>
<td>y</td>
<td>y</td>
<td>y</td>
</tr>
<tr class="row-even"><td>Exponential Power</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-odd"><td>Exponential Weibull</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-even"><td>Birnbaum-Sanders</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-odd"><td>Fisher-Snedecor</td>
<td>n</td>
<td>y</td>
<td>y</td>
</tr>
<tr class="row-even"><td>Fisk/Log-Logistic</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-odd"><td>Folded Cauchy</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-even"><td>Folded Normal</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-odd"><td>Frechet</td>
<td>y</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-even"><td>Gamma</td>
<td>y</td>
<td>y</td>
<td>y</td>
</tr>
<tr class="row-odd"><td>Gen. Exponential</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-even"><td>Gen. Extreme Value</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-odd"><td>Gen. Gamma</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-even"><td>Gen. Half-Logistic</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-odd"><td>Gilbrat</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-even"><td>Truncated Gumbel</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-odd"><td>Gumbel</td>
<td>y</td>
<td>y</td>
<td>y</td>
</tr>
<tr class="row-even"><td>Hypergeometric Secant</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-odd"><td>Inverse-Normal</td>
<td>n</td>
<td>y</td>
<td>n</td>
</tr>
<tr class="row-even"><td>Kumaraswamy</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-odd"><td>Laplace</td>
<td>n</td>
<td>y</td>
<td>y</td>
</tr>
<tr class="row-even"><td>Levy</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-odd"><td>Log-Gamma</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-even"><td>Log-Laplace</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-odd"><td>Log-Normal</td>
<td>y</td>
<td>y</td>
<td>y</td>
</tr>
<tr class="row-even"><td>Log-Uniform</td>
<td>y</td>
<td>y</td>
<td>y</td>
</tr>
<tr class="row-odd"><td>Logistic</td>
<td>n</td>
<td>y</td>
<td>y</td>
</tr>
<tr class="row-even"><td>Lomax</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-odd"><td>Maxwell</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-even"><td>Mielke&#8217;s Beta-Kappa</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-odd"><td>Nakagami</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-even"><td>Non-Central Chi-Squared</td>
<td>n</td>
<td>y</td>
<td>y</td>
</tr>
<tr class="row-odd"><td>Non-Central Student-T</td>
<td>n</td>
<td>y</td>
<td>y</td>
</tr>
<tr class="row-even"><td>Non-central F</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-odd"><td>Normal</td>
<td>y</td>
<td>y</td>
<td>y</td>
</tr>
<tr class="row-even"><td>Pareto (First kind)</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-odd"><td>Power Log-Normal</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-even"><td>Power Normal</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-odd"><td>Raised Cosine</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-even"><td>Rayleigh</td>
<td>n</td>
<td>y</td>
<td>y</td>
</tr>
<tr class="row-odd"><td>Reciprocal</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-even"><td>Rice</td>
<td>n</td>
<td>y</td>
<td>n</td>
</tr>
<tr class="row-odd"><td>Right-skewed Gumbel</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-even"><td>Student-T</td>
<td>n</td>
<td>y</td>
<td>y</td>
</tr>
<tr class="row-odd"><td>Trapezoidal</td>
<td>n</td>
<td>y</td>
<td>n</td>
</tr>
<tr class="row-even"><td>Triangle</td>
<td>y</td>
<td>y</td>
<td>y</td>
</tr>
<tr class="row-odd"><td>Truncated Exponential</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-even"><td>Truncated Normal</td>
<td>n</td>
<td>y</td>
<td>y</td>
</tr>
<tr class="row-odd"><td>Tukey-Lamdba</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-even"><td>Uniform</td>
<td>y</td>
<td>y</td>
<td>y</td>
</tr>
<tr class="row-odd"><td>Wald</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-even"><td>Weibull</td>
<td>y</td>
<td>y</td>
<td>y</td>
</tr>
<tr class="row-odd"><td>Wigner</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-even"><td>Wrapped Cauchy</td>
<td>n</td>
<td>n</td>
<td>y</td>
</tr>
<tr class="row-odd"><td>Zipf-Mandelbrot</td>
<td>n</td>
<td>y</td>
<td>n</td>
</tr>
</tbody>
</table>
<p>The Chaospy software supports in addition custom distributions through
the function <code class="docutils literal"><span class="pre">cp.constructor</span></code>.  To illustrate its use, consider the
simple example of a uniform random variable on the interval <span class="math">\([lo,up]\)</span>.
The minimal input to create such a distribution is</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">Uniform</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">constructor</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">cdf</span><span class="o">=</span><span class="k">lambda</span> <span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">lo</span><span class="p">,</span><span class="n">up</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">lo</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">up</span><span class="o">-</span><span class="n">lo</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">bnd</span><span class="o">=</span><span class="k">lambda</span> <span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">lo</span><span class="p">,</span><span class="n">up</span><span class="p">:</span> <span class="p">(</span><span class="n">lo</span><span class="p">,</span><span class="n">up</span><span class="p">)</span> <span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">uniform</span> <span class="o">=</span> <span class="n">Uniform</span><span class="p">(</span><span class="n">lo</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">up</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>Here, the two provided arguments are a cumulative distribution
function (<code class="docutils literal"><span class="pre">cdf</span></code>), and a boundary interval function
(<code class="docutils literal"><span class="pre">bnd</span></code>), respectively.
The <code class="docutils literal"><span class="pre">cp.constructor</span></code> function also takes several
optional arguments to provide extra functionality.
For example, the inverse of the cumulative distribution function &#8211;
the point percentile function - can be provided through the
<code class="docutils literal"><span class="pre">ppf</span></code> keyword.
If this function is not provided, the software will automatically
approximate it using the method described in the section <a class="reference internal" href="#sec-invrosenblatt"><span class="std std-ref">Numerical Estimation of Inverse Rosenblatt Transformations</span></a>.</p>
</div>
<div class="section" id="copulas">
<span id="sec-copulas"></span><h2>Copulas<a class="headerlink" href="#copulas" title="Permalink to this headline">¶</a></h2>
<p>Dakota and Turns do not support the Rosenblatt transformation
applied to multivariate distributions with dependencies.  Instead, the
two packages model dependencies using copulas
<a class="reference internal" href="._chaospy005.html#ref16" id="id7">[Ref16]</a>.  A copula consists of stochastically
independent multivariate distributions made dependent using a
parameterized function <span class="math">\(g\)</span>.  Since the Rosenblatt transformation is
general purpose, it is possible to construct any copula
directly. However, this can quickly become a very cumbersome task
since each copula must be decomposed individually for each combination
of independent distributions and parameterization of <span class="math">\(g\)</span>.</p>
<p>To simplify the user&#8217;s efforts, Chaospy has dedicated constructors
that can reformulate a copula coupling into a Rosenblatt
transformation.  This is done following the work of Lee
<a class="reference internal" href="._chaospy005.html#ref17" id="id8">[Ref17]</a> and approximated using finite differences.
The implementation is based of the software toolbox RoseDist
<a class="reference internal" href="._chaospy005.html#ref18" id="id9">[Ref18]</a>.  In practice, this approach allow
copulas to be defined in a Rosenblatt transformation setting.  For
example, to construct a bivariate normal distribution with a Clayton
copula in Chaospy, we do the following:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">joint</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">J</span><span class="p">(</span><span class="n">cp</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">cp</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clayton</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">Clayton</span><span class="p">(</span><span class="n">joint</span><span class="p">,</span> <span class="n">theta</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>A list of supported copulas is provided below.
It shows that Turns supports 7 methods, Chaospy
6, while Dakota offers 1 method.</p>
<table border="1" class="docutils">
<colgroup>
<col width="58%" />
<col width="14%" />
<col width="12%" />
<col width="16%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head"><strong>Supported Copulas</strong></th>
<th class="head">Dakota</th>
<th class="head">Turns</th>
<th class="head">Chaospy</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Ali-Mikhail-Haq</td>
<td>no</td>
<td>yes</td>
<td>yes</td>
</tr>
<tr class="row-odd"><td>Clayton</td>
<td>no</td>
<td>yes</td>
<td>yes</td>
</tr>
<tr class="row-even"><td>Farlie-Gumbel-Morgenstein</td>
<td>no</td>
<td>yes</td>
<td>no</td>
</tr>
<tr class="row-odd"><td>Frank</td>
<td>no</td>
<td>yes</td>
<td>yes</td>
</tr>
<tr class="row-even"><td>Gumbel</td>
<td>no</td>
<td>yes</td>
<td>yes</td>
</tr>
<tr class="row-odd"><td>Joe</td>
<td>no</td>
<td>no</td>
<td>yes</td>
</tr>
<tr class="row-even"><td>Minimum</td>
<td>no</td>
<td>yes</td>
<td>no</td>
</tr>
<tr class="row-odd"><td>Normal/Nataf</td>
<td>yes</td>
<td>yes</td>
<td>yes</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="variance-reduction-techniques">
<span id="sec-monte-carlo"></span><h2>Variance Reduction Techniques<a class="headerlink" href="#variance-reduction-techniques" title="Permalink to this headline">¶</a></h2>
<p>As noted in the beginning of the section <a class="reference internal" href="#sec-dist"><span class="std std-ref">Modelling Random Variables</span></a>, by generating
samples <span class="math">\(\{\bm Q_k\}_{k\in I_K}\)</span> and evaluating the response function
<span class="math">\(f\)</span>, it is possible to draw inference upon <span class="math">\(Y\)</span> without knowledge about
<span class="math">\(p_{Y}\)</span>, through Monte Carlo simulation.  Unfortunately, the number of
samples <span class="math">\(K\)</span> to achieve reasonable accuracy can often be very high, so
if <span class="math">\(f\)</span> is assumed to be computationally expensive, the number of
samples needed frequently make Monte Carlo simulation infeasible for
practical applications.  As a way to mitigate this problem, it is
possible to modify <span class="math">\(\{\bm Q_k\}_{k\in I_K}\)</span> from traditional
pseudo-random samples, so that the accuracy increases.  Schemes that
select non-traditional samples for <span class="math">\(\{\bm Q_k\}_{k\in I_K}\)</span> to
increase accuracy are known as emph{variance reduction techniques}.
A list of such techniques are presented in the tables below,
and they show that Dakota, Turns and Chaospy
support 4, 7, and 7 variance reduction techniques, respectively.</p>
<table border="1" class="docutils">
<colgroup>
<col width="63%" />
<col width="13%" />
<col width="10%" />
<col width="15%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head"><strong>Quasi-Monte Carlo Scheme</strong></th>
<th class="head">Dakota</th>
<th class="head">Turns</th>
<th class="head">Chaospy</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Faure sequence <a class="reference internal" href="._chaospy005.html#ref19" id="id10">[Ref19]</a></td>
<td>no</td>
<td>yes</td>
<td>no</td>
</tr>
<tr class="row-odd"><td>Halton sequence <a class="reference internal" href="._chaospy005.html#ref20" id="id11">[Ref20]</a></td>
<td>yes</td>
<td>yes</td>
<td>yes</td>
</tr>
<tr class="row-even"><td>Hammersley sequence <a class="reference internal" href="._chaospy005.html#ref21" id="id12">[Ref21]</a></td>
<td>yes</td>
<td>yes</td>
<td>yes</td>
</tr>
<tr class="row-odd"><td>Haselgrove sequence <a class="reference internal" href="._chaospy005.html#ref22" id="id13">[Ref22]</a></td>
<td>no</td>
<td>yes</td>
<td>no</td>
</tr>
<tr class="row-even"><td>Korobov latice <a class="reference internal" href="._chaospy005.html#ref23" id="id14">[Ref23]</a></td>
<td>no</td>
<td>no</td>
<td>yes</td>
</tr>
<tr class="row-odd"><td>Niederreiter sequence <a class="reference internal" href="._chaospy005.html#ref24" id="id15">[Ref24]</a></td>
<td>no</td>
<td>yes</td>
<td>no</td>
</tr>
<tr class="row-even"><td>Sobol sequence <a class="reference internal" href="._chaospy005.html#ref25" id="id16">[Ref25]</a></td>
<td>no</td>
<td>yes</td>
<td>yes</td>
</tr>
</tbody>
</table>
<table border="1" class="docutils">
<colgroup>
<col width="62%" />
<col width="11%" />
<col width="13%" />
<col width="13%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head"><strong>Other Methods</strong></th>
<th class="head">Dakota</th>
<th class="head">Turns</th>
<th class="head">Chaospy</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Antithetic variables <a class="reference internal" href="._chaospy005.html#ref01" id="id17">[Ref01]</a></td>
<td>no</td>
<td>no</td>
<td>yes</td>
</tr>
<tr class="row-odd"><td>Importance sampling <a class="reference internal" href="._chaospy005.html#ref01" id="id18">[Ref01]</a></td>
<td>yes</td>
<td>yes</td>
<td>yes</td>
</tr>
<tr class="row-even"><td>Latin Hypercube sampling <a class="reference internal" href="._chaospy005.html#ref26" id="id19">[Ref26]</a></td>
<td>yes</td>
<td>limited</td>
<td>yes</td>
</tr>
</tbody>
</table>
<p>One of the more popular variance reduction technique is the
<em>quasi-Monte Carlo scheme</em> <a class="reference internal" href="._chaospy005.html#ref01" id="id20">[Ref01]</a>.  The
method consists of selecting the samples <span class="math">\(\{\bm Q_k\}_{k\in I_K}\)</span> to
be a low-discrepancy sequence instead of pseudo-random samples.  The
idea is that samples placed with a given distance from each other
increase the coverage over the sample space, requiring fewer samples
to reach a given accuracy.  For example, if standard Monte Carlo
requires <span class="math">\(10^6\)</span> samples for a given accuracy, quasi-Monte Carlo can
often get away with only <span class="math">\(10^3\)</span>.  Note that this would break some of
the statistical properties of the samples <a class="reference internal" href="._chaospy005.html#ref27" id="id21">[Ref27]</a>.</p>
<p>Most of the theory on quasi-Monte Carlo methods focuses on generating
samples on the unit hypercube <span class="math">\([0,1]^N\)</span>.  The option to generate
samples directly on to other distributions exists, but is often very
limited.  To the authors&#8217; knowledge, the only viable method for
including most quasi-Monte Carlo methods into the vast majority of
non-standard probability distributions, is through the Rosenblatt
transformation.  Since Chaospy is built around the Rosenblatt
transformation, it has the novel feature of supporting quasi-Monte
Carlo methods for all probability distributions.  Turns and Dakota
only support Rosenblatt transformations for independent variables and
the Normal copula.</p>
<p>Sometimes the quasi-Monte Carlo method is infeasible because the
forward model is too computationally costly.  The next section
describes polynomial chaos expansions, which often require far fewer
samples than the quasi-Monte Carlo method for the same amount of
accuracy.</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <center>
            <p class="logo"><a href="http://cbc.simula.no/" title="Go to Center for Biomedical Computing">
              <img class="logo" src="_static/cbc_logo.png" alt="Logo"/>
            </a></p>
            </center>
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Modelling Random Variables</a><ul>
<li><a class="reference internal" href="#rosenblatt-transformation">Rosenblatt Transformation</a></li>
<li><a class="reference internal" href="#numerical-estimation-of-inverse-rosenblatt-transformations">Numerical Estimation of Inverse Rosenblatt Transformations</a></li>
<li><a class="reference internal" href="#constructing-distributions">Constructing Distributions</a></li>
<li><a class="reference internal" href="#copulas">Copulas</a></li>
<li><a class="reference internal" href="#variance-reduction-techniques">Variance Reduction Techniques</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="._chaospy002.html"
                        title="previous chapter">A Glimpse of Chaospy in Action</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="._chaospy004.html"
                        title="next chapter">Polynomial Chaos Expansions</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/._chaospy003.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="._chaospy004.html" title="Polynomial Chaos Expansions"
             >next</a> |</li>
        <li class="right" >
          <a href="._chaospy002.html" title="A Glimpse of Chaospy in Action"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">Chaospy Documentation</a> &raquo;</li> 
      </ul>
    </div>
<div class="wrapper">
  <div class="footer">
    <a href="http://cbc.simula.no"><img src="_static/cbc_banner.png" width="100%"><a>
    <br />
    <br />
      &copy;2015, Jonathan Feinberg, Hans Petter Langtangen. Released under CC Attribution-NonCommercial 4.0 license.
  </div>
</div>

  </body>
</html>