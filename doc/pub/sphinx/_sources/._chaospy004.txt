.. !split

.. _sec:chaos:

Polynomial Chaos Expansions
===========================

Polynomial chaos expansions represent a collection of methods that can
be considered a subset of polynomial approximation methods, but
particularly designed for uncertainty quantification.  A general
polynomial approximation can be defined as

.. _Eq:eq_approx_poly:

.. math::
   :label: eq_approx_poly
        
        
            \hat f(\boldsymbol{x}, t, \boldsymbol{Q}) =
            \sum_{n\in I_N} c_n(\boldsymbol{x}, t) \boldsymbol{\Phi}_n(\boldsymbol{Q}) \qquad
            I_N = \{0,\dots,N\},
        

where :math:`\{c_n\}_{n\in I_N}` are coefficients (often known as Fourier coefficients)
and :math:`\{\boldsymbol{\Phi}_n\}_{n\in I_N}` are polynomials.
If :math:`\hat f` is a good approximation of :math:`f`, it is possible to
either infer statistical properties of :math:`\hat f` analytically
or through cheap numerical computations where :math:`\hat f` is used
as a surrogate for :math:`f`.

A polynomial chaos expansion is defined as a polynomial approximation,
as in :eq:`eq_approx_poly`, where the polynomials
:math:`\{\boldsymbol{\Phi}_n\}_{n\in I_N}` are orthogonal on a custom weighted
function space :math:`L_Q`:

.. _Eq:eq_orthogonal:

.. math::
   :label: eq_orthogonal
        
            \left\langle\boldsymbol{\Phi}_n,\boldsymbol{\Phi}_m\right\rangle =
            \mathbb{E}\!\left[ \boldsymbol{\Phi}_n(\boldsymbol{Q}) \boldsymbol{\Phi}_m(\boldsymbol{Q}) \right] =
            \int\cdots\int \boldsymbol{\Phi}_n(\boldsymbol{q}) \boldsymbol{\Phi}_m(\boldsymbol{q})
                p_{\boldsymbol{Q}}(\boldsymbol{q}) {\rm d} \boldsymbol{q} = 0,\quad
            n\neq m.
            
        

As a side note, it is worth noting that in parallel with polynomial
chaos expansions, there also exists an alternative collocation method
based on multivariate Lagrange polynomials [Ref28]_.
This method is supported by Dakota and Chaospy, but not Turns.

To generate a polynomial chaos expansion, we must first calculate the
polynomials :math:`\{\boldsymbol{\Phi}_n\}_{n\in I_N}` such that the orthogonality
property in :eq:`eq_orthogonal` is satisfied.  This will be the topic
of the section :ref:`sec:orthogonal` In the section :ref:`sec:spectral` we show
how to estimate the coefficients :math:`\{c_n\}_{n\in I_N}`.  Last, in
the section :ref:`sec:descriptive`, tools used to quantify uncertainty in
polynomial chaos expansions will be discussed.

.. _sec:orthogonal:

Orthogonal Polynomials Construction
-----------------------------------

From :eq:`eq_orthogonal` it follows that the orthogonality property
is not in general transferable between distributions, since a new set
of polynomials has to be constructed for each :math:`p_{\boldsymbol{Q}}`.  The
easiest approach to construct orthogonal polynomials is to identify
the probability density :math:`p_{\boldsymbol{Q}}` in the so-called Askey-Wilson
scheme [Ref29]_.  The polynomials can then be picked
from a list, or be built from analytical components.  The continuous
distributions supported in the scheme include the standard normal,
gamma, beta, and uniform distributions respectively through the
Hermite, Laguerre, Jacobi, and Legendre polynomial expansion.  All the
three mentioned software toolboxes support these expansions.

Moving beyond the standard collection of the Askey-Wilson scheme, it
is possible to create custom orthogonal polynomials, both analytically
and numerically.  Unfortunately, most methods involving finite
precision arithmetics are ill-posed, making a numerical approach quite
a challenge [Ref30]_.  This section explores
the various approaches for constructing polynomial expansions.  A full
list of methods is found in the table below.  It shows that Dakota,
Turns and Chaospy support 4, 3 and 5 orthogonalisation methods,
respectively.

===============================  ======  =====  =======  
  **Orthogonalization Method**   Dakota  Turns  Chaospy  
===============================  ======  =====  =======  
Askey-Wilson Scheme [Ref29]_      yes     yes     yes    
Bertran recursion [Ref31]_         no      no     yes    
Cholesky Decomposition [Ref14]_    no      no     yes    
Discretized Stieltjes [Ref32]_    yes      no     yes    
Modified Chebyshev [Ref32]_       yes     yes      no    
Modified Gram-Schmidt [Ref32]_    yes     yes     yes    
===============================  ======  =====  =======  

Looking beyond an analytical approach, the most popular method for
constructing orthogonal polynomials is the discretized Stieltjes
procedure [Ref33]_.  As far as the authors know,
it is the only truly numerically stable method for orthogonal
polynomial construction.  It is based upon one-dimensional recursion
coefficients that are estimated using numerical integration.
Unfortunately, the method is only applicable in the multivariate case
if the components of :math:`p_{\boldsymbol{Q}}` are stochastically independent.

Generalized Polynomial Chaos Expansions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

One approach to model densities with stochastically dependent
components numerically, is to reformulate the uncertainty problem as a
set of independent components through generalised polynomial chaos
expansion [Ref34]_.  As described in detail in
the section :ref:`sec:rosenblatt`, a Rosenblatt transformation allows for
the mapping between any domain and the unit hypercube :math:`[0,1]^D`.  With
a double transformation we can reformulate the response function :math:`f`
as

.. math::
        
            f(\boldsymbol{x}, t, \boldsymbol{Q}) =
            f(\boldsymbol{x}, t, T_{\boldsymbol{Q}}^{-1}(T_{\boldsymbol{R}}(\boldsymbol{R}))) &\approx
            \hat f(\boldsymbol{x}, t, \boldsymbol{R}) =
            \sum_{n\in I_N} c_n(\boldsymbol{x}, t)\boldsymbol{\Phi}_n(\boldsymbol{R}),
        

where :math:`\boldsymbol{R}` is any random variable drawn from :math:`p_{\boldsymbol{R}}`, which
for simplicity is chosen to consists of independent components.
Also, :math:`\{\boldsymbol{\Phi}_n\}_{n\in I_N}`
is constructed to be orthogonal with respect to :math:`L_{\boldsymbol{R}}`, not
:math:`L_{\boldsymbol{Q}}`.
In any case, :math:`\boldsymbol{R}` is either selected from the Askey-Wilson
scheme, or calculated using the discretized Stieltjes procedure.
We remark that the accuracy of the approximation deteriorate if the
transformation composition :math:`T_{\boldsymbol{Q}}^{-1}\circ T_{\boldsymbol{R}}` is not
smooth [Ref34]_.

Dakota, Turns, and Chaospy all support generalized polynomial chaos
expansions for independent stochastic variables and the Normal/Nataf
copula listed in the table in the section :ref:`sec:copulas`.  Since Chaospy
has the Rosenblatt transformation underlying the computational
framework, generalized polynomial chaos expansions are in fact
available for all densities.

The Direct Multivariate Approach
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Given that both the density :math:`p_{\boldsymbol{Q}}` has stochastically dependent
components, and the transformation composition :math:`T^{-1}_{\boldsymbol{Q}}\circ
T_{\boldsymbol{R}}` is not smooth, it is still possible to generate orthogonal
polynomials numerically.  As noted above, most methods are numerically
unstable, and the accuracy in the orthogonality can deteriorate with
polynomial order, but the methods can still be useful
[Ref14]_.  In the table in the section :ref:`sec:orthogonal`, only Chaospy's implementation of Bertran's
recursion method [Ref31]_, Cholesky decomposition
[Ref35]_ and modified Gram-Schmidt
orthogonalization [Ref32]_ support construction
of orthogonal polynomials for multivariate dependent densities
directly.

Custom Polynomial Expansions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In the most extreme cases, an automated numerical method is
insufficient.  Instead, a polynomial expansion has to be constructed
manually.  User-defined expansions can be created conveniently, as
demonstrated in the next example involving a second-order Hermite
polynomial expansion, orthogonal with respect to the normal density
[Ref29]_:

.. math::
        
            \left\{\boldsymbol{\Phi}_n\right\}_{n\in I_6} =
            \left\{ 1, Q_0, Q_1, Q_0^2-1, Q_0Q_1, Q_1^2-1 \right\}
        

The relevant Chaospy code for creating this polynomial expansion looks like

.. code-block:: python

        >>> q0, q1 = cp.variable(2)
        >>> phi = cp.Poly([1, q0, q1, q0**2-1, q0*q1, q1**2-1])
        >>> print phi
        [1, q0, q1, q0^2-1, q0q1, -1+q1^2]

Chaospy contains a collection of tools to manipulate and create
polynomials, see the table below.

================  ======================================  
  **Function**               **Description**              
================  ======================================  
``all``           Test all coefficients for non-zero      
``any``           Test any coefficients for non-zero      
``around``        Round to a given decimal                
``asfloat``       Set coefficients type as float          
``asint``         Set coefficient type as int             
``basis``         Create monomial basis                   
``cumprod``       Cumulative product                      
``cumsum``        Cumulative sum                          
``cutoff``        Truncate polynomial order               
``decompose``     Convert from series to sequence         
``diag``          Construct or extract diagonal           
``differential``  Differential operator                   
``dot``           Dot-product                             
``flatten``       Flatten an array                        
``gradient``      Gradient (or Jacobian) operator         
``hessian``       Hessian operator                        
``inner``         Inner product                           
``mean``          Average                                 
``order``         Extract polynomial order                
``outer``         Outer product                           
``prod``          Product                                 
``repeat``        Repeat polynomials                      
``reshape``       Reshape axes                            
``roll``          Roll polynomials                        
``rollaxis``      Roll axis                               
``rolldim``       Roll the dimension                      
``std``           Empirical standard deviation            
``substitute``    Variable substitution                   
``sum``           Sum along an axis                       
``swapaxes``      Interchange two axes                    
``swapdim``       Swap the dimensions                     
``trace``         Sum along the diagonal                  
``transpose``     Transpose the coefficients              
``tril``          Extract lower triangle of coefficients  
``tricu``         Extract cross-diagonal upper triangle   
``var``           Empirical variance                      
``variable``      Simple polynomial constructor           
================  ======================================  

One thing worth noting is that polynomial chaos expansions suffers
from the curse of dimensionality: The number of terms grows
exponentially with the number of dimensions [Ref36]_.  As a
result, Chaospy does not support neither high dimensional nor infinite
dimensional problems (random fields).  One approach to address such
problems with polynomial chaos expansion is to first reduce the number
of dimension through techniques like Karhunen-Loeve expansions
[Ref37]_.  If software implementations of such
methods can be provided, the user can easily extend Chaospy to high
and infinite dimensional problems.

Chaospy includes operators such as the expectation operator :math:`\mathbb
E`.  This is a helpful tool to ensure that the constructed polynomials
are orthogonal, as defined in :eq:`eq_orthogonal`.  To verify that
two elements in ``phi`` are indeed orthogonal under the standard
bivariate normal distribution, one writes

.. code-block:: python

        >>> dist = cp.J(cp.Normal(0,1), cp.Normal(0,1))
        >>> print cp.E(phi[3]*phi[5], dist)
        0.0

More details of operators used to perform uncertainty analysis
are given in the section :ref:`sec:descriptive`.

\color{black}

.. _sec:spectral:

Calculating Coefficients
------------------------

There are several methodologies for estimating the coefficients
:math:`\{c_n\}_{n\in I_N}`, typically categorized either as non-intrusive or
intrusive, where non-intrusive means that the computational procedures
only requires evaluation of :math:`f` (i.e., software for :math:`f` can be reused
as a black box). Intrusive methods need to incorporate information
about the underlying forward model in the computation of the
coefficients. In case of forward models based on differential
equations, one performs a Galerkin formulation for the coefficients in
probability space, leading effectively to a :math:`D`-dimensional
differential equation problem in this space
[Ref38]_.  Back et al. [Ref39]_
demonstrated that the computational cost of such an intrusive Galerkin
method in some cases was higher than some non-intrusive methods.  None
of the three toolboxes discussed in this paper have support for
intrusive methods.

Within the realm of non-intrusive methods, there are in principle two
viable methodologies available: pseudo-spectral projection
[Ref40]_ and the point collocation method
[Ref41]_.  The former applies a numerical
integration scheme to estimate Fourier coefficients, while the latter
solves a linear system arising from a statistical regression
formulation.  Dakota and Chaospy support both methodologies, while
Turns only supports point collocation.  We shall now discuss the
practical, generic implementation of these two methods in Chaospy.

Integration Methods
-------------------

The pseudo-spectral projection method is based on a standard least
squares minimization in the weighted function space :math:`L_{\boldsymbol{Q}}`.
Since the polynomials are orthogonal in this space, the associated
linear system is diagonal, which allows a closed-form expression for
the Fourier coefficients. The expression involves high-dimensional
integrals in :math:`L_{\boldsymbol{Q}}`. Numerical integration is then required,

.. _Eq:eq_quadrature:

.. math::
   :label: eq_quadrature
        
            
            c_n = \frac{\mathbb{E}\!\left[Y \boldsymbol{\Phi}_n\right]}{\mathbb{E}\!\left[\boldsymbol{\Phi}_n^2\right]} =
            \frac{1}{\mathbb{E}\!\left[\boldsymbol{\Phi}_n^2\right]}\int\cdots\int
            p_{\boldsymbol{Q}}(\boldsymbol{q})
            f(\boldsymbol{x}, t, \boldsymbol{q}) \boldsymbol{\Phi}_n(\boldsymbol{q}){\rm d}\boldsymbol{q} 
        

.. math::
          \nonumber
            \approx \frac{1}{\mathbb{E}\!\left[\boldsymbol{\Phi}_n^2\right]}
            \sum_{k\in I_K} w_k p_{\boldsymbol{Q}}(\boldsymbol{q}_k)
            f(\boldsymbol{x}, t, \boldsymbol{q}_k)
            \boldsymbol{\Phi}_n(\boldsymbol{q}_k) 
            I_K = \{0,\dots,K-1\},
        

where :math:`w_k` are weights and :math:`\boldsymbol{q}_k` nodes in a quadrature scheme.
Note that :math:`f` is only evaluated for the nodes :math:`\boldsymbol{q}_k`, and these
evaluations can be made once. Thereafter, one can experiment with the
polynomial order since any :math:`c_n` depends on the same evaluations of :math:`f`.

The table below shows the various quadrature schemes offered by Dakota
and Chaospy (recall that Turns does not support pseudo-spectral
projection).

====================================  ======  =====  =======  
   **Node and Weight Generators**     Dakota  Turns  Chaospy  
====================================  ======  =====  =======  
Clenshaw-Curtis quadrature [Ref42]_    yes      no     yes    
Cubature rules [Ref43]_                yes      no      no    
Gauss-Legendre quadrature [Ref44]_     yes      no     yes    
Gauss-Patterson quadrature [Ref45]_    yes      no     yes    
Genz-Keister quadrature [Ref46]_       yes      no     yes    
Leja quadrature [Ref47]_                no      no     yes    
Monte Carlo integration [Ref01]_       yes      no     yes    
Optimal Gaussian quadrature [Ref44]_   yes      no     yes    
====================================  ======  =====  =======  

All techniques for generating nodes and weights in Chaospy are
available through the ``cp.generate_quadrature`` function.  Suppose we
want to generate optimal Gaussian quadrature nodes for the normal
distribution. We then write

.. code-block:: python

        >>> nodes, weights = cp.generate_quadrature(
        ...             3, cp.Normal(0, 1), rule="Gaussian")
        >>> print nodes
        [[-2.33441422 -0.74196378  0.74196378  2.33441422]]
        >>> print weights
        [ 0.04587585  0.45412415  0.45412415  0.04587585]

Most quadrature schemes are designed for univariate problems.  To
extend a univariate scheme to the multivariate case, integration rules
along each axis can be combined using a tensor product.
Unfortunately, such a product suffers from the curse of dimensionality
and becomes a very costly integration procedure for large :math:`D`.  In
higher-dimensional problems one can replace the full tensor product by
a Smolyak sparse grid [Ref48]_.  The method works
by taking multiple lower order tensor product rules and joining them
together.  If the rule is nested, i.e., the same samples found at a
low order are also included at higher order, the number of evaluations
can be further reduced.  Another feature is to add anisotropy such
that some dimensions are sampled more than others
[Ref49]_.  In addition to the tensor product
rules, there are a few native multivariate cubature rules that allow
for low order multivariate integration [Ref43]_.
Both Dakota and Chaospy also support the Smolyak sparse grid and
anisotropy.

Chaospy has support for construction of custom integration rules
defined by the user.  The ``cp.rule_generator`` function can be used to
join a list of univariate rules using tensor grid or Smolyak sparse
grid.  For example, consider the trapezoid rule:

.. code-block:: python

        >>> def trapezoid(n):
        ...    X = np.linspace(0, 1, n+1)
        ...    W = np.ones(n+1)/n
        ...    W[0] *= 0.5; W[-1] *= 0.5
        ...    return X, W
        ...
        >>> nodes, weights = trapezoid(2)
        >>> print nodes
        [ 0.   0.5  1. ]
        >>> print weights
        [ 0.25  0.5   0.25]

The ``cp.rule_generator`` function takes positional arguments, each
representing a univariate rule.  To generate a rule for the
multivariate case, with the same one-dimensional rule along two axes,
we do the following:

.. code-block:: python

        >>> mvtrapezoid = cp.rule_generator(trapezoid, trapezoid)
        >>> nodes, weights = mvtrapezoid(2, sparse=True)
        >>> print nodes
        [[ 0.   0.5  1.   0.   0.   1. ]
         [ 0.   0.   0.   0.5  1.   1. ]]
        >>> print weights
        [ 0.     0.25   0.125  0.25   0.125  0.25 ]

Software for constructing and executing a general-purpose integration
scheme is useful for several computational components in uncertainty
quantification.  For example, in the section :ref:`sec:orthogonal` when
constructing orthogonal polynomials using raw statistical moments, or
calculating discretized Stieltjes' recurrence coefficients, numerical
integration is relevant.  Like the ``ppf`` function noted in the section :ref:`sec:variable`, the moments and recurrence coefficients can be
added directly into each distribution.  However, when these are not
available, Chaospy will automatically estimate missing information by
quadrature rules, using the ``cp.generate_quadrature`` function
described above.

To compute the Fourier coefficients and the polynomial chaos
expansion, we use the ``cp.fit_quadrature`` function.  It takes four
arguments: the set of orthogonal polynomials, quadrature nodes,
quadrature weights, and the user's function for evaluating the forward
model (to be executed at the quadrature nodes).  Note that in the case
of the discretized Stieltjes method discussed in the section :ref:`sec:orthogonal`, the nominator :math:`\mathbb{E}\!\left[{\boldsymbol{\Phi}_n^2}\right]` in
:eq:`eq_quadrature` can be calculated more accurately using
recurrence coefficients [Ref32]_.  Special
numerical features like this can be added by including optional
arguments in ``cp.fit_quadrature``.

.. _sec:ptcolloc:

Point Collocation
-----------------

The other non-intrusive approach to estimate the coefficients
:math:`\{c_k\}_{k\in I_K}` is the point collocation method.  One way of
formulating the method is to require the polynomial expansion to equal
the model evaluations at a set of collocation nodes :math:`\{\boldsymbol{q}_k\}_{k\in
I_K}`, resulting in an over-determined set of linear equations for the
Fourier coefficients:

.. _Eq:eq_pcm:

.. math::
   :label: eq_pcm
        
        
        \left[\begin{array}{ccc}
                \Phi_0(\boldsymbol{q}_0) & \cdots & \Phi_N(\boldsymbol{q}_0) \\
                \vdots &  & \vdots \\
                \Phi_0(\boldsymbol{q}_{K-1}) & \cdots & \Phi_N(\boldsymbol{q}_{K-1})
        \end{array}\right]
        \left[\begin{array}{c}
        c_0 \\ \vdots \\ c_N
        \end{array}\right] =
        \left[\begin{array}{c}
                f(\boldsymbol{q}_0) \\ \vdots \\
                f(\boldsymbol{q}_{K-1})
        \end{array}\right],
        

Unlike pseudo spectral projection, the locations of the collocation
nodes are not required to follow any integration rule.  Hosder
[Ref41]_ showed that the solution using Hammersley
samples from quasi-Monte Carlo samples resulted in more stable results
than using conventional pseudo-random samples.  In other words, well
placed collocation nodes might increase the accuracy.  In Chaospy
these collocation nodes can be selected from integration rules or from
pseudo-random samples from Monte Carlo simulation, as discussed in
the section :ref:`sec:monte_carlo`.  In addition, the software accepts user
defined strategies for choosing the sampling points.  Turns also
allows for user-defined points, while Dakota has its predefined
strategies.

The obvious way to solve the over-determined system in :eq:`eq_pcm`
is to use least squares minimization, which resembles the standard
statistical linear regression approach of fitting a polynomial to a
set of data points.  However, from a numerical point of view, this
might not be the best strategy.  If the numerical stability of the
solution is low, it might be prudent to use Tikhonov regularization
[Ref50]_, or if the problem is so large that the number
of coefficients is very high, it might be useful to force some of the
coefficients to be zero through least angle regression
[Ref51]_.  Being able to run and compare alternative
methods is important in many problems to see if numerical stability is
a potential problem. The table below lists the regression
methods offered by Dakota, Turns, and Chaospy.

===============================================  ======  =====  =======  
             **Regression Schemes**              Dakota  Turns  Chaospy  
===============================================  ======  =====  =======  
Basis Pursuit [Ref52]_                            yes      no      no    
Bayesian Auto. Relevance Determination [Ref53]_    no      no     yes    
Bayesian ridge [Ref54]_                            no      no     yes    
Elastic Net [Ref55]_                              yes      no     yes    
Forward Stagewise [Ref56]_                         no     yes      no    
Least Absolute Shrinkage and Selection [Ref51]_   yes     yes     yes    
Least Angle and Shrinkage with AIC/BIC [Ref57]_    no      no     yes    
Least Squares Minimization                        yes     yes     yes    
Orthogonal matching pursuit [Ref58]_              yes      no     yes    
Singular Value Decomposition                       no     yes      no    
Tikhonov Regularization [Ref50]_                   no      no     yes    
===============================================  ======  =====  =======  

Generating a polynomial chaos expansion using linear
regression is done using Chaospy's ``cp.fit_regression`` function.
It takes the same arguments as ``cp.fit_quadrature``, except that
quadrature weights are omitted, and optional arguments
define the rule used to optimize :eq:`eq_pcm`.

Model Evaluations
-----------------

Irrespectively of the method used to estimate the coefficients :math:`c_k`,
the user is left with the job to evaluate the forward model (response
function) :math:`f`, which is normally by far the most computing-intensive
part in uncertainty quantification.  Chaospy does not impose any
restriction on the simulation code used to compute the forward
model. The only requirement is that the user can provide an array of
values of :math:`f` at the quadrature or collocation nodes.  Chaospy users
will usually wrap any complex simulation code for :math:`f` in a Python
function ``f(q)``, where ``q`` is a node in probability space (i.e., ``q``
contains values of the uncertain parameters in the problem).  For
example, for pseudo-spectral projection, samples of :math:`f` can be created
as

.. code-block:: python

        samples = [f(node) for node in nodes.T]

or perhaps done in parallel if :math:`f` is time consuming to evaluate:

.. code-block:: python

        import multiprocessing as mp
        pool = mp.Pool(mp.cpu_count())
        samples = pool.map(f, nodes.T)

The evaluation of all the :math:`f` values can also be done in parallel with
MPI in a distributed way on a cluster using the Python module like
``mpi4py``.  Both Dakota and Turns support parallel evaluation of :math:`f`
values, but the feature is embeded into the code, potentially limiting
the customization options of the parallelization.

Extension of polynomial expansions
----------------------------------

There is much literature that extends on the theory of polynomial
chaos expansion [Ref36]_.  For example, Isukapalli showed
that the accuracy of a polynomial expansion could be increased by
using partial derivatives of the model response
[Ref59]_.  This theory is only directly
supported by Dakota.  In Turns and Chaospy the support is indirect by
allowing the user to add the feature manually.

To be able to incorporate partial derivatives of the response, the
partial derivative of the polynomial expansion must be available as
well.  In both Turns and Chaospy, the derivative of a polynomial can
be generated easily.  This derivative can then be added to the
expansion, allowing us to incorporate Isukapalli's theory in practice.
This is just an example on how manipulation of the polynomial
expansions and model approximations can overcome the lack of support
for a particular feature from the literature.

To be able to support many current and possible future extensions of
polynomial chaos, a large collection of tools for manipulating
polynomials must be available.  In Dakota, no such tools exist from a
user perspective.  In Turns, there is support for some arithmetic
operators in addition to the derivative.  In Chaospy, however, the
polynomial generated for the model response is of the same type as the
polynomials generated in the sections :ref:`sec:orthogonal` and
:ref:`sec:spectral`, and the rich set of manipulations of polynomials is
then available for :math:`\hat f` as well.

Beyond the analytical tools for statistical analysis of :math:`\hat f`,
either from the toolbox or custom ones by the user, there are many
statistical metrics that cannot easily be expressed as simple
closed-form formulas.  Such metrics include confidence intervals,
sensitivity indices, p-values in hypothesis testing, to mention a few.
In those scenarios, it makes sense to perform a secondary uncertainty
analysis through Monte Carlo simulation.  Evaluating the approximation
:math:`\hat f` is normally computationally much cheaper than evaluating the
full forward model :math:`f`, thus allowing a large number of Monte Carlo
samples within a cheap computational budget.  This type of secondary
simulations are done automatically in the background in Dakota and
Turns, while Chaospy does not feature automated tools for secondary
Monte Carlo simulation.  Instead, Chaospy allows for simple and
computationally cheap generation of pseudo-random samples, as
described in the section :ref:`sec:monte_carlo`, such that the user can
easily put together a tailored Monte Carlo simulation to meet the
needs at hand.  Within a few lines of Python code, the samples can be
analyzed with the standard Numpy and the Scipy libraries
[Ref60]_ or with more specialized statistical libraries
like Pandas [Ref09]_, Scikit-learn
[Ref61]_, Scikit-statsmodel
[Ref62]_, and Python's interface to the rich R
environment for statistical computing.  For example, for the specific
:math:`\hat f` function illustrated above, the following code computes a 90
percent confidence interval, based on :math:`10^5` pseudo-random samples and
Numpy's functionality for finding percentiles in discrete data:

.. code-block:: python

        >>> q_samples = cp.Normal(0,1).sample(10**5)
        >>> samples = f_approx(*q_samples)
        >>> p05, p95 = np.percentile(samples, [5, 95], axis=-1)
        >>> print p05[:3]
        [ 1.         1.0000004  1.0000016]
        >>> print p95[:3]
        [ 1.          1.00038886  1.00155544]

Since the type of statistical analysis of :math:`\hat f` often strongly
depends on the physical problem at hand, we believe that the ability
to quickly compose custom solutions by putting together basic building
blocks is very useful in uncertainty quantification. This is yet
another example of the need for a package with a strong focus on easy
customization.

.. _sec:descriptive:

Descriptive Tools
-----------------

The last step in uncertainty quantification based on polynomial chaos
expansions is to quantify the uncertainty.  In polynomial chaos
expansion this is done by using the uncertainty in the model
approximation ``f_approx`` as a substiute for the uncertainty in the
model :math:`f`.

For the most popular statistical metrics, like mean, variance,
correlation, a polynomial chaos expansion allows for analytical
analysis, which is easy to calculate and has high accuracy.  This
property is reflected in all the three toolboxes.  To calculate the
expected value, variance and correlation of a simple (here univariate)
polynomial approximation ``f_approx``, with a normally distributed
:math:`\xi_0` variable, we can with Chaospy write

.. code-block:: python

        >>> f_approx = fit_quadrature(orth, nodes, weights, samples)
        >>> print f_approx
        [q0, q0^2, q0^3]
        >>> dist = Normal(0,1)
        >>> print E(f_approx, dist)
        [ 0.  1.  0.]
        >>> print Var(f_approx, dist)
        [  1.   2.  15.]
        >>> print Corr(f_approx, dist)
        [[ 1.          0.          0.77459667]
         [ 0.          1.          0.        ]
         [ 0.77459667  0.          1.        ]]

A list of supported analytical metrics is listed in the table below.

=======================  ======  =====  =======  
       **Method**        Dakota  Turns  Chaospy  
=======================  ======  =====  =======  
Covariance/Correlation    yes     yes     yes    
Expected value            yes     yes     yes    
Conditional expectation    no      no     yes    
Kurtosis                  yes     yes     yes    
Sensitivity index         yes     yes     yes    
Skewness                  yes     yes     yes    
Variance                  yes     yes     yes    
=======================  ======  =====  =======  

