.. !split

.. _sec:dist:

Modelling Random Variables
==========================

.. _sec:rosenblatt:

Rosenblatt Transformation
-------------------------

Numerical methods for uncertainty quantification need to generate
pseudo-random realizations

.. math::
        
            \{\boldsymbol{Q}_k\}_{k\in I_K} \qquad I_K = \{1,\dots,K\},
        

from the density :math:`p_{\boldsymbol{Q}}`.  Each :math:`\boldsymbol{Q}\in \{\boldsymbol{Q}_k\}_{k\in I_K}`
is multivariate with the number of dimensions :math:`D>1`.  Generating
realizations from a given density :math:`p_{\boldsymbol{Q}}` is often non-trivial, at
least when :math:`D` is large.  A very common assumption made in uncertainty
quantification is that each dimension in :math:`\boldsymbol{Q}` consists of
stochastically independent components.  Stochastic independence allows
for a joint sampling scheme to be reduced to a series of univariate
samplings, drastically reducing the complexity of generating a sample
:math:`\boldsymbol{Q}`.

Unfortunately, the assumption of independence does not always hold in
practice.  We have examples from many research fields where stochastic
dependence must be assumed, including modelling of climate
[Ref10]_, iron-ore minerals [Ref11]_,
finance [Ref12]_, and ion channel densities in
detailed neuroscience models [Ref13]_.  There also
exists examples where introducing dependent random variables is
beneficial for the modelling process, even though the original input
was stochastically independent [Ref14]_.  In
any cases, modelling of stochastically dependent variables are
required to perform uncertainty quantification adequately.  A strong
feature of Chaospy is its support for stochastic dependence.

All random samples are in Chaospy generated using Rosenblatt
transformations :math:`T_{\boldsymbol{Q}}` [Ref15]_.  It allows
for a random variable :math:`\boldsymbol{U}`, generated uniformly on a unit hypercube
:math:`[0,1]^D`, to be transformed into :math:`\boldsymbol{Q} = T^{-1}_{\boldsymbol{Q}}(\boldsymbol{U})`,
which behaves as if it were drawn from the density :math:`p_{\boldsymbol{Q}}`.  It is
easy to generate pseudo-random samples from a uniform distribution,
and the Rosenblatt transformation can then be used as a method for
generating samples from arbitrary densities.

The Rosenblatt transformation can be derived as follows.  Consider a
probability decomposition, for example for a bivariate random variable
:math:`\boldsymbol{Q}=(Q_0, Q_1)`:

.. _Eq:eq_bivariate:

.. math::
   :label: eq_bivariate
        
        
            p_{Q_0,Q_1}(q_0,q_1) =
            p_{Q_0}(q_0) p_{Q_1\mid Q_0}(q_1 \mid q_0),
        

were :math:`p_{Q_0}` is an marginal density function, and
:math:`p_{Q_1\mid Q_0}` is a conditional density.
For the multivariate case, the density decomposition
will have the form

.. _Eq:eq_decompostion:

.. math::
   :label: eq_decompostion
        
            p_{\boldsymbol{Q}}(\boldsymbol{q}) =
            \prod_{d=0}^{D-1} p_{Q^{\prime}_d}(q^{\prime}_d),
            
        

where

.. _Eq:eq_notation:

.. math::
   :label: eq_notation
        
        
            Q^\prime_d = Q_d \mid Q_0, \dots, Q_{d-1} \qquad
            q^\prime_d = q_d \mid q_0,\dots,q_{d-1}
        

denotes that :math:`Q_d` and :math:`q_d` are dependent on all components with
lower indices.
A forward Rosenblatt transformation can then be defined as

.. _Eq:eq_forward:

.. math::
   :label: eq_forward
        
        
            T_{\boldsymbol{Q}}(\boldsymbol{q}) =
            (F_{Q_0^{\prime}}(q_0^{\prime}),
            \dots,F_{Q_{D-1}^{\prime}}(q_{D-1}^{\prime})),
        

where :math:`F_{Q_d^{\prime}}` is the cumulative distribution function:

.. _Eq:eq_general_cdf:

.. math::
   :label: eq_general_cdf
        
            F_{Q_d^{\prime}}(q_d^{\prime}) =
            \int_{-\infty}^{q_d} \!\!
            p_{Q_d^{\prime}}(r\mid q_0,\dots,q_{d-1})
            {\rm d} r.
            
        

This transformation is bijective, so it is always possible to define
the inverse Rosenblatt transformation :math:`T_{\boldsymbol{Q}}^{-1}` in a similar
fashion.

.. _sec:invRosenblatt:

Numerical Estimation of Inverse Rosenblatt Transformations
----------------------------------------------------------

To implement the Rosenblatt transformation in practice, we need to
identify the inverse transform :math:`T_{\boldsymbol{Q}}^{-1}`.  Unfortunately,
:math:`T_{\boldsymbol{Q}}` is often non-linear without a closed-form formula, making
analytical calculations of the transformation's inverse difficult.  In
the scenario where we do not have a symbolic representation of the
inverse transformation, a numerical scheme has to be employed.  To the
authors' knowledge, there are no standards for defining such a
numerical scheme.  The following paragraphs therefore describe our
proposed method for calculating the inverse transformation
numerically.

The problem of calculating the inverse transformation :math:`T^{-1}_{\boldsymbol{Q}}`
can, by decomposing the definition of the forward Rosenblatt
transformation in :eq:`eq_forward`, be reformulated as

.. math::
        
            F_{Q^\prime_d}^{-1}(u\mid q_0,\dots,q_{d-1}) &=
            \left\{ r : F_{Q^\prime_d}(r\mid q_0,\dots,q_{d-1})=u \right\}
            \qquad d=0,\dots,D-1.
        

In other words, the challenge of calculating the inverse
transformation can be reformulated as a series of one dimensional
root-finding problems.  In Chaospy, these roots are found by employing
a Newton-Raphson scheme.  However, to ensure convergence, the scheme
is coupled with a bisection method.  The bisection method is
applicable here since the problem is one-dimensional and the functions
of interest are by definition monotone.  When the Newton-Raphson
method fails to converge at an increment, a bisection step gives the
Newton-Raphson a new start location away from the previous
location. This algorithm ensures fast and reliable convergence towards
the root.

The Newton-Raphson-bisection hybrid method is implemented as follows.
The initial values are the lower and upper bounds :math:`[lo_0, up_0]`.  If
:math:`p_{Q^{\prime}_d}` is unbound, the interval is selected such that it
approximately covers the density.  For example for a standard normal
random variable, which is unbound, the interval :math:`[-7.5,7.5]` will
approximately cover the whole density with an error about :math:`10^{-14}`.
The algorithm starts with a Newton-Raphson increment, using the
initial value :math:`r_0=(up_0-lo_0) u + lo_0`:

.. _Eq:eq_newton:

.. math::
   :label: eq_newton
        
            
            r_{k+1} = r_k - \frac{F_{Q^\prime_d}
            (r_k\mid q_0,\dots,q_{d-1})-u}{
            p_{Q^\prime_d}
            (r_k\mid q_0,\dots,q_{d-1})},
        

where the density :math:`p_{Q^\prime_d}` can be approximated using finite
differences.
If the new value does not fall in the interval :math:`[lo_k, up_k]`, this
proposed value is rejected, and is instead replaced with a
bisection increment:

.. _Eq:eq_bisection:

.. math::
   :label: eq_bisection
        
            
            r_{k+1} = \frac{up_k+lo_k}{2}.
        

In either case, the bounds are updated according to

.. _Eq:eq_bounds:

.. math::
   :label: eq_bounds
        
        (lo_{k+1},up_{k+1}) =
        \left\lbrace\begin{array}{ll}
                (lo_{k}, r_{k+1}) &
                F_{Q^\prime_d}(r_{k+1}\mid q_0,\dots,q_{d-1}) > u \\
                (r_{k+1}, up_{k}) &
                F_{Q^\prime_d}(r_{k+1}\mid q_0,\dots,q_{d-1}) < u
        \end{array}\right.
        
        

The algorithm repeats the steps in :eq:`eq_newton`,
:eq:`eq_bisection` and :eq:`eq_bounds`, until the residual
:math:`|F_{Q^\prime_d}(r_k\mid q_0,\dots,q_{d-1})-u|` is sufficiently
small.

The described algorithm overcomes one of the challenges of
implementing Rosenblatt transformations in practice: how to calculate
the inverse transformation.  Another challenge is how to construct a
transformation in the first place.  This is the topic of the next
section.

.. _sec:variable:

Constructing Distributions
--------------------------

The backbone of distributions in Chaospy is the Rosenblatt
transformation :math:`T_{\boldsymbol{Q}}`.  The method, as described in the previous
section, assumes that :math:`p_{\boldsymbol{Q}}` is known to be able to perform the
transformation and its inverse.  In practice, however, we first need
to construct :math:`p_{\boldsymbol{Q}}`, before the transformation can be used.  This
can be a challenging task, but in Chaospy a lot of effort has been put
into constructing novel tools for making the process as flexible and
painless as possible.  In essence, users can create their own custom
multivariate distributions using a new methodology as described next.

Following the definition in :eq:`eq_forward`, each Rosenblatt
transformation consists of a collection of conditional distributions.
We express all conditionality through distribution parameters.  For
example, the location parameter of a normal distribution can be set to
be uniformly distributed, say on :math:`[-1,1]`.  The following interactive
Python code defines a normal variable with a normally distributed
mean:

.. code-block:: python

        >>> uniform = cp.Uniform(lo=-1, up=1)
        >>> normal = cp.Normal(mu=uniform, sigma=0.1)

We now have two stochastic variables, ``uniform`` and ``normal``,
whose joint bivariate distribution can be constructed
through the ``cp.J`` function:

.. code-block:: python

        >>> joint = cp.J(uniform, normal)

The software will, from this minimal formulation, try to sort out
the dependency ordering and construct the full Rosenblatt
transformation.
The only requirement is that a decomposition as
in :eq:`eq_decompostion` is in fact possible.
The result is a fully functioning forward and inverse Rosenblatt
transformation. The following code evaluates the forward
transformation (the density)
at :math:`(1,0.9)`, the inverse transformation at :math:`(0.4, 0.6)`,
and draws a random sample from the joint distribution:

.. code-block:: python

        >>> print joint.fwd([1, 0.9])
        [ 1.          0.15865525]
        >>> print joint.inv([0.4, 0.6])
        [-0.2        -0.17466529]
        >>> print joint.sample()
        [-0.05992158 -0.07456064]

Distributions in higher dimensions are trivially obtained by including more
arguments to the ``cp.J`` function.

As an alternative to the explicit formulation of dependency through
distribution parameters, it is also possible to construct dependencies
implicitly through arithmetic operators.  For example, it is possible
to recreate the example above using addition of stochastic variables
instead of letting a distribution parameter be stochastic. More
precisely, we have a uniform variable on :math:`[-1,1]` and a normally
distributed variable with location at :math:`x=0`. Adding the uniform
variable to the normal variable creates a new normal variable with
stochastic location:

.. code-block:: python

        >>> uniform = Uniform(lo=-1, up=1)
        >>> normal0 = Normal(mu=0, scale=0.1)
        >>> normal = normal0 + uniform
        >>> joint = J(uniform, normal)

As before, the software automatically sorts the dependency
ordering from the context.
Here, since the uniform variable is present as first argument, the
software recognises the second argument as a normal distribution,
conditioned on the uniform distribution, and not the other way
around.

Another favorable feature in Chaospy is that multiple transformations
can be stacked on top of each other.  For example, consider the
example of a multivariate log-normal random variable :math:`\boldsymbol{Q}` with
three dependent components.  (Let us ignore for a moment the fact that
Chaospy already offers such a distribution.) Trying to decompose this
distribution is a very cumbersome task if performed manually.
However, this process can be drastically simplified through variable
transformations, for which Chaospy has strong support.  A log-normal
distribution, for example, can be expressed as

.. math::
        
            \boldsymbol{Q} = e^{\boldsymbol{Z} L + \boldsymbol{b}},
        

where :math:`\boldsymbol{Z}` are standard normal variables, and :math:`L` and :math:`\boldsymbol{b}`
are predefined matrix and vector, respectively.
To implement this particular transformation, we only
have to write

.. code-block:: python

        >>> Z = cp.J(cp.Normal(0,1), cp.Normal(0,1), cp.Normal(0,1))
        >>> Q = e**(Z*L + b)

The resulting distribution is fully functional multivariate
log-normal, assuming :math:`L` and :math:`\boldsymbol{b}` are properly defined.

One obvious prerequisite for using univariate distributions to create
conditionals and multivariate distributions, is the availability of
univariate distributions.  Since the univariate distribution is the
fundamental building block, Chaospy offers a large collection of 64
univariate distributions.  They are all listed in table below.  The
titles 'D', 'T' and 'C' represents Dakota, Turns and Chaospy
respectively. The elements 'y' and 'n' represent the answers 'yes' and
'no' indicating if the distribution is supported or not.  The list
shows that Dakota's support is limited to 11 distributions, and Turns
has a collection of 26 distributions.

=======================  =  =  =  
    **Distribution**     D  T  C  
=======================  =  =  =  
Alpha                    n  n  y  
Anglit                   n  n  y  
Arcsinus                 n  n  y  
Beta                     y  y  y  
Brandford                n  n  y  
Burr                     n  y  y  
Cauchy                   n  n  y  
Chi                      n  y  y  
Chi-Square               n  y  y  
Double Gamma             n  n  y  
Double Weibull           n  n  y  
Epanechnikov             n  y  y  
Erlang                   n  n  y  
Exponential              y  y  y  
Exponential Power        n  n  y  
Exponential Weibull      n  n  y  
Birnbaum-Sanders         n  n  y  
Fisher-Snedecor          n  y  y  
Fisk/Log-Logistic        n  n  y  
Folded Cauchy            n  n  y  
Folded Normal            n  n  y  
Frechet                  y  n  y  
Gamma                    y  y  y  
Gen. Exponential         n  n  y  
Gen. Extreme Value       n  n  y  
Gen. Gamma               n  n  y  
Gen. Half-Logistic       n  n  y  
Gilbrat                  n  n  y  
Truncated Gumbel         n  n  y  
Gumbel                   y  y  y  
Hypergeometric Secant    n  n  y  
Inverse-Normal           n  y  n  
Kumaraswamy              n  n  y  
Laplace                  n  y  y  
Levy                     n  n  y  
Log-Gamma                n  n  y  
Log-Laplace              n  n  y  
Log-Normal               y  y  y  
Log-Uniform              y  y  y  
Logistic                 n  y  y  
Lomax                    n  n  y  
Maxwell                  n  n  y  
Mielke's Beta-Kappa      n  n  y  
Nakagami                 n  n  y  
Non-Central Chi-Squared  n  y  y  
Non-Central Student-T    n  y  y  
Non-central F            n  n  y  
Normal                   y  y  y  
Pareto (First kind)      n  n  y  
Power Log-Normal         n  n  y  
Power Normal             n  n  y  
Raised Cosine            n  n  y  
Rayleigh                 n  y  y  
Reciprocal               n  n  y  
Rice                     n  y  n  
Right-skewed Gumbel      n  n  y  
Student-T                n  y  y  
Trapezoidal              n  y  n  
Triangle                 y  y  y  
Truncated Exponential    n  n  y  
Truncated Normal         n  y  y  
Tukey-Lamdba             n  n  y  
Uniform                  y  y  y  
Wald                     n  n  y  
Weibull                  y  y  y  
Wigner                   n  n  y  
Wrapped Cauchy           n  n  y  
Zipf-Mandelbrot          n  y  n  
=======================  =  =  =  

The Chaospy software supports in addition custom distributions through
the function ``cp.constructor``.  To illustrate its use, consider the
simple example of a uniform random variable on the interval :math:`[lo,up]`.
The minimal input to create such a distribution is

.. code-block:: python

        >>> Uniform = cp.constructor(
        ...     cdf=lambda self,x,lo,up: (x-lo)/(up-lo),
        ...     bnd=lambda self,x,lo,up: (lo,up) )
        >>> uniform = Uniform(lo=-1, up=1)

Here, the two provided arguments are a cumulative distribution
function (``cdf``), and a boundary interval function
(``bnd``), respectively.
The ``cp.constructor`` function also takes several
optional arguments to provide extra functionality.
For example, the inverse of the cumulative distribution function --
the point percentile function - can be provided through the
``ppf`` keyword.
If this function is not provided, the software will automatically
approximate it using the method described in the section :ref:`sec:invRosenblatt`.

.. _sec:copulas:

Copulas
-------

Dakota and Turns do not support the Rosenblatt transformation
applied to multivariate distributions with dependencies.  Instead, the
two packages model dependencies using copulas
[Ref16]_.  A copula consists of stochastically
independent multivariate distributions made dependent using a
parameterized function :math:`g`.  Since the Rosenblatt transformation is
general purpose, it is possible to construct any copula
directly. However, this can quickly become a very cumbersome task
since each copula must be decomposed individually for each combination
of independent distributions and parameterization of :math:`g`.

To simplify the user's efforts, Chaospy has dedicated constructors
that can reformulate a copula coupling into a Rosenblatt
transformation.  This is done following the work of Lee
[Ref17]_ and approximated using finite differences.
The implementation is based of the software toolbox RoseDist
[Ref18]_.  In practice, this approach allow
copulas to be defined in a Rosenblatt transformation setting.  For
example, to construct a bivariate normal distribution with a Clayton
copula in Chaospy, we do the following:

.. code-block:: python

        >>> joint = cp.J(cp.Normal(0,1), cp.Normal(0,1))
        >>> clayton = cp.Clayton(joint, theta=2)

A list of supported copulas is provided below.
It shows that Turns supports 7 methods, Chaospy
6, while Dakota offers 1 method.

=========================  ======  =====  =======  
  **Supported Copulas**    Dakota  Turns  Chaospy  
=========================  ======  =====  =======  
Ali-Mikhail-Haq              no     yes     yes    
Clayton                      no     yes     yes    
Farlie-Gumbel-Morgenstein    no     yes      no    
Frank                        no     yes     yes    
Gumbel                       no     yes     yes    
Joe                          no      no     yes    
Minimum                      no     yes      no    
Normal/Nataf                yes     yes     yes    
=========================  ======  =====  =======  

.. _sec:monte_carlo:

Variance Reduction Techniques
-----------------------------

As noted in the beginning of the section :ref:`sec:dist`, by generating
samples :math:`\{\boldsymbol{Q}_k\}_{k\in I_K}` and evaluating the response function
:math:`f`, it is possible to draw inference upon :math:`Y` without knowledge about
:math:`p_{Y}`, through Monte Carlo simulation.  Unfortunately, the number of
samples :math:`K` to achieve reasonable accuracy can often be very high, so
if :math:`f` is assumed to be computationally expensive, the number of
samples needed frequently make Monte Carlo simulation infeasible for
practical applications.  As a way to mitigate this problem, it is
possible to modify :math:`\{\boldsymbol{Q}_k\}_{k\in I_K}` from traditional
pseudo-random samples, so that the accuracy increases.  Schemes that
select non-traditional samples for :math:`\{\boldsymbol{Q}_k\}_{k\in I_K}` to
increase accuracy are known as \emph{variance reduction techniques}.
A list of such techniques are presented in the tables below,
and they show that Dakota, Turns and Chaospy
support 4, 7, and 7 variance reduction techniques, respectively.

==============================  ======  =====  =======  
 **Quasi-Monte Carlo Scheme**   Dakota  Turns  Chaospy  
==============================  ======  =====  =======  
Faure sequence [Ref19]_           no     yes      no    
Halton sequence [Ref20]_         yes     yes     yes    
Hammersley sequence [Ref21]_     yes     yes     yes    
Haselgrove sequence [Ref22]_      no     yes      no    
Korobov latice [Ref23]_           no      no     yes    
Niederreiter sequence [Ref24]_    no     yes      no    
Sobol sequence [Ref25]_           no     yes     yes    
==============================  ======  =====  =======  

=================================  ======  =======  =======  
        **Other Methods**          Dakota   Turns   Chaospy  
=================================  ======  =======  =======  
Antithetic variables [Ref01]_        no       no      yes    
Importance sampling [Ref01]_        yes      yes      yes    
Latin Hypercube sampling [Ref26]_   yes    limited    yes    
=================================  ======  =======  =======  

One of the more popular variance reduction technique is the
*quasi-Monte Carlo scheme* [Ref01]_.  The
method consists of selecting the samples :math:`\{\boldsymbol{Q}_k\}_{k\in I_K}` to
be a low-discrepancy sequence instead of pseudo-random samples.  The
idea is that samples placed with a given distance from each other
increase the coverage over the sample space, requiring fewer samples
to reach a given accuracy.  For example, if standard Monte Carlo
requires :math:`10^6` samples for a given accuracy, quasi-Monte Carlo can
often get away with only :math:`10^3`.  Note that this would break some of
the statistical properties of the samples [Ref27]_.

Most of the theory on quasi-Monte Carlo methods focuses on generating
samples on the unit hypercube :math:`[0,1]^N`.  The option to generate
samples directly on to other distributions exists, but is often very
limited.  To the authors' knowledge, the only viable method for
including most quasi-Monte Carlo methods into the vast majority of
non-standard probability distributions, is through the Rosenblatt
transformation.  Since Chaospy is built around the Rosenblatt
transformation, it has the novel feature of supporting quasi-Monte
Carlo methods for all probability distributions.  Turns and Dakota
only support Rosenblatt transformations for independent variables and
the Normal copula.

Sometimes the quasi-Monte Carlo method is infeasible because the
forward model is too computationally costly.  The next section
describes polynomial chaos expansions, which often require far fewer
samples than the quasi-Monte Carlo method for the same amount of
accuracy.

